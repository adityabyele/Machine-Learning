{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, lda):    \n",
    "    XT = X.transpose()\n",
    "    d = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    a = np.dot(X,XT)\n",
    "    a = 2*a.diagonal()\n",
    "    w = np.random.rand(d)\n",
    "    #w = np.zeros(d)\n",
    "    b = 0\n",
    "    In = np.ones(n)\n",
    "    Id = np.ones(d)\n",
    "    delta = 0.001\n",
    "    prevRmse = 0\n",
    "    currRmse = rmse(X, w, b, y)\n",
    "    while(abs(prevRmse - currRmse) > delta):\n",
    "        r = (y - X.T.dot(w)) - b\n",
    "        bOld = b\n",
    "        b = np.sum(r+b)/float(n)\n",
    "        r = r - b + bOld;        \n",
    "        wkOld = 0\n",
    "        for k in range(0,d):\n",
    "            ck = 2*(X[k].dot((r.reshape(r.shape[0], 1) + (w[k]*(X[k].T)))))            \n",
    "            wkOld = w[k]\n",
    "            if(ck < (-1)*lda):\n",
    "                w[k] = (ck + lda)/a[k]\n",
    "            elif((ck <= lda) and (ck >= (-1)*lda)):\n",
    "                w[k] = 0\n",
    "            else: \n",
    "                w[k]  = (ck - lda)/a[k]\n",
    "            r = r.reshape(r.shape[0], 1) - (w[k] - wkOld)*X[k].T\n",
    "            r = np.squeeze(np.asarray(r))            \n",
    "        prevRmse = currRmse            \n",
    "        currRmse = rmse(X, w, b, y)\n",
    "    return w,b,currRmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(X, w, b, y):\n",
    "    r = y - X.T.dot(w) - b\n",
    "    r_sqr = np.square(r)\n",
    "    sqrSum = np.sum(r_sqr)\n",
    "    n = X.shape[0]\n",
    "    tmp = sqrSum/(n*1.0)\n",
    "    import math\n",
    "    return math.sqrt(tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(X, w, b, y, lda):\n",
    "    r = y - X.T.dot(w) - b\n",
    "    r_sqr = np.square(r)\n",
    "    sqrSum = np.sum(r_sqr)\n",
    "    w = np.absolute(w)\n",
    "    reg = np.sum(w)\n",
    "    reg = lda*reg\n",
    "    return sqrSum + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "lambda 5863.72996396\n",
      "trainRmse 54.0689955653\n",
      "Sparsity pattern 1\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 2931.86498198\n",
      "trainRmse 34.1680863999\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 1465.93249099\n",
      "trainRmse 17.1521854437\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 732.966245495\n",
      "trainRmse 8.71088277149\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 366.483122748\n",
      "trainRmse 4.61525885293\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 183.241561374\n",
      "trainRmse 2.7668664024\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 91.6207806869\n",
      "trainRmse 2.06011573524\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 45.8103903434\n",
      "trainRmse 1.81307214824\n",
      "Sparsity pattern 20\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 22.9051951717\n",
      "trainRmse 1.66055405049\n",
      "Sparsity pattern 40\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 11.4525975859\n",
      "trainRmse 1.55323819647\n",
      "Sparsity pattern 63\n",
      "*****************************************\n",
      "*****************************************\n",
      "Final Sparsity pattern 63\n",
      "Final recall 1.0\n",
      "Final precision 0.15873015873\n",
      "*****************************************\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "    \n",
    "    n= 250\n",
    "    d =80\n",
    "    X = np.random.standard_normal(size = (d,n))\n",
    "    k = 10\n",
    "    wOpt = np.zeros(d)\n",
    "    wOpt[:k] = 10\n",
    "    e = np.random.normal(0, 1, size = (n))\n",
    "    bOpt = 0\n",
    "    y = X.T.dot(wOpt) + bOpt + e    \n",
    "    ldaMax = 2*np.amax(abs(X.dot(y - (np.sum(y)/float(n)))))    \n",
    "    lda = ldaMax\n",
    "    X1 = sp.csr_matrix(X)\n",
    "    train(X1, y, lda)\n",
    "    nonZeroList = []\n",
    "    precisionList = []\n",
    "    recallList = []\n",
    "    lambdaList = []\n",
    "    prevRmse = 999999999999999\n",
    "    currRmse = 99999999999999\n",
    "    \n",
    "    for i in range(10):\n",
    "        w,b,trainRmse = train(X1, y, lda)                        \n",
    "        np.count_nonzero(w[:k])\n",
    "        correctW = np.count_nonzero(w[:k])        \n",
    "        prevRmse = currRmse\n",
    "        currRmse = rmse(X,w,b,y)\n",
    "        total = np.count_nonzero(w)\n",
    "        if(total != 0):\n",
    "            precision = correctW/float(total)\n",
    "        else:\n",
    "            precision = 1\n",
    "        precisionList.append(precision)\n",
    "        recall = correctW/float(k)\n",
    "        recallList.append(recall)\n",
    "        lambdaList.append(lda)\n",
    "        print \"*****************************************\"\n",
    "        print \"lambda\", lda\n",
    "        print \"trainRmse\", trainRmse\n",
    "        print \"Sparsity pattern\", np.count_nonzero(w)\n",
    "        print \"*****************************************\"\n",
    "        lda = lda/2\n",
    "    \n",
    "    correctW = np.count_nonzero(w[:k])\n",
    "    total = np.count_nonzero(w)\n",
    "    precision = correctW/float(total)    \n",
    "    recall = correctW/float(k)  \n",
    "    print \"*****************************************\"\n",
    "    print \"Final Sparsity pattern\", np.count_nonzero(w)\n",
    "    print \"Final recall\", recall\n",
    "    print \"Final precision\", precision\n",
    "    print \"*****************************************\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5863.7299639612138, 2931.8649819806069, 1465.9324909903034, 732.96624549515172, 366.48312274757586, 183.24156137378793, 91.620780686893966, 45.810390343446983, 22.905195171723491, 11.452597585861746]\n"
     ]
    }
   ],
   "source": [
    "print lambdaList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66c731bb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lambdaList, precisionList)\n",
    "plt.plot(lambdaList, recallList)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('others')\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(ax.get_xlim()[::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFDYZsBaypoYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66c731b610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nonZeroList)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "lambda 4.37239662251\n",
      "trainRmse 15.852381855\n",
      "Sparsity pattern 80\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 2.18619831126\n",
      "trainRmse 15.852521912\n",
      "Sparsity pattern 80\n",
      "*****************************************\n"
     ]
    }
   ],
   "source": [
    "    nonZeroList = []\n",
    "    precisionList = []\n",
    "    recallList = []\n",
    "    lambdaList = []\n",
    "    \n",
    "    prevRmse = 999999999999999\n",
    "    currRmse = 99999999999999    \n",
    "    while(prevRmse > currRmse):\n",
    "        w,b,trainRmse = train(X1, y, lda)                        \n",
    "        np.count_nonzero(w[:k])\n",
    "        correctW = np.count_nonzero(w[:k])        \n",
    "        prevRmse = currRmse\n",
    "        currRmse = rmse(X,w,b,y)\n",
    "        total = np.count_nonzero(w)\n",
    "        if(total != 0):\n",
    "            precision = correctW/float(total)\n",
    "        else:\n",
    "            precision = 1\n",
    "        precisionList.append(precision)\n",
    "        recall = correctW/float(k)\n",
    "        recallList.append(recall)\n",
    "        lambdaList.append(lda)\n",
    "        print \"*****************************************\"\n",
    "        print \"lambda\", lda\n",
    "        print \"trainRmse\", trainRmse\n",
    "        print \"Sparsity pattern\", np.count_nonzero(w)\n",
    "        print \"*****************************************\"\n",
    "        lda = lda/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "lambda 573098.770105\n",
      "trainRmse 549.071489916\n",
      "Sparsity pattern 1\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 286549.385053\n",
      "trainRmse 329.024064853\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 143274.692526\n",
      "trainRmse 165.228529537\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 71637.3462632\n",
      "trainRmse 84.0320597306\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 35818.6731316\n",
      "trainRmse 44.7392685705\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 17909.3365658\n",
      "trainRmse 27.1414833139\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 8954.6682829\n",
      "trainRmse 20.5042251509\n",
      "Sparsity pattern 10\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 4477.33414145\n",
      "trainRmse 18.2223733474\n",
      "Sparsity pattern 18\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 2238.66707072\n",
      "trainRmse 17.0185988079\n",
      "Sparsity pattern 37\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 1119.33353536\n",
      "trainRmse 16.3062735989\n",
      "Sparsity pattern 55\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 559.666767681\n",
      "trainRmse 16.0005498383\n",
      "Sparsity pattern 67\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 279.833383841\n",
      "trainRmse 15.8954244933\n",
      "Sparsity pattern 74\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 139.91669192\n",
      "trainRmse 15.8638313666\n",
      "Sparsity pattern 76\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 69.9583459601\n",
      "trainRmse 15.8551357762\n",
      "Sparsity pattern 79\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 34.9791729801\n",
      "trainRmse 15.8530527055\n",
      "Sparsity pattern 80\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 17.48958649\n",
      "trainRmse 15.8527351678\n",
      "Sparsity pattern 80\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 8.74479324502\n",
      "trainRmse 15.8524179929\n",
      "Sparsity pattern 80\n",
      "*****************************************\n",
      "*****************************************\n",
      "lambda 4.37239662251\n",
      "trainRmse 15.8525692292\n",
      "Sparsity pattern 80\n",
      "*****************************************\n",
      "Rmse 15.8524910821\n",
      "Sparsity pattern 80\n",
      "precision 0.125\n",
      "recall 1.0\n",
      "[  9.96152856   9.93283076   9.92079767  10.1104572    9.9557904\n",
      "  10.04300535   9.89248816   9.92838134   9.87393605  10.00134941]\n"
     ]
    }
   ],
   "source": [
    "    n= 250\n",
    "    d =80\n",
    "    X = np.random.normal(0, 10, size = (d,n))\n",
    "    k = 10\n",
    "    wOpt = np.zeros(d)\n",
    "    wOpt[:k] = 10\n",
    "    e = np.random.normal(0, 10, size = (n))\n",
    "    bOpt = 0\n",
    "    y = X.T.dot(wOpt) + bOpt + e\n",
    "    \n",
    "    ldaMax = 2*np.amax(abs(X.dot(y - (np.sum(y)/float(n)))))    \n",
    "    lda = ldaMax\n",
    "    X1 = sp.csr_matrix(X)\n",
    "    train(X1, y, lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rmse 15.8528659128\n",
      "Sparsity pattern 80\n",
      "precision 0.125\n",
      "recall 1.0\n",
      "[  9.96083208   9.9340541    9.92131921  10.10886832   9.95723088\n",
      "  10.04508636   9.8917376    9.92701605   9.8723529   10.00018128]\n"
     ]
    }
   ],
   "source": [
    "    lda = 20#2.18619831126#4.37239662251#1.61958470205#3.23916940411#0.809792351027#1.70821731642#3.41643463284#1.70821731642 #3.41643463284\n",
    "    w,b,trainRmse = train(X1, y, lda)                        \n",
    "    print \"Rmse\", trainRmse\n",
    "#     for i in range(10):\n",
    "#         w,b,trainRmse = train(X1, y, lda)                        \n",
    "#         np.count_nonzero(w[:k])\n",
    "#         correctW = np.count_nonzero(w[:k])\n",
    "#         total = np.count_nonzero(w)\n",
    "#         if(total != 0):\n",
    "#             precision = correctW/float(total)\n",
    "#         precisionList.append(precision)\n",
    "#         recall = correctW/float(k)\n",
    "#         recallList.append(recall)\n",
    "#         lambdaList.append(lda)\n",
    "#         lda = lda/2\n",
    "    print \"Sparsity pattern\", np.count_nonzero(w)\n",
    "    correctW = np.count_nonzero(w[:k])\n",
    "    total = np.count_nonzero(w)\n",
    "    precision = correctW/float(total)\n",
    "    print \"precision\", precision\n",
    "    recall = correctW/float(k)\n",
    "    print \"recall\", recall\n",
    "    print w[:k]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
